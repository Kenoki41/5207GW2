---
title: "Group Assignment 2"
author: "Group 2"
date: "2021/2/3"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, messages = FALSE)

library (tidyverse)

```


# Introduction and Theory of the Our Assignment

In this assignment, the 3 factors we use are:
  1. median_age_persons
  2. median_rent_weekly
  3. median_tot_prsnl_inc_weekly
  
There's the reason:
  The median is a very meaningful data in population data, and the median reflects the middle level of the data. It divides the data into two parts, one is below the intermediate level and the other is above the intermediate level. 
  
  Here we choose the median age. This is the mainstay of the social population. Those below the median age are young people who have voting rights, and those above the median age are elderly people. Considering that every age group may have a biased attitude towards the program of a certain political party, the median age here is a meaningful data.
  
  Like the median age, the median weekly rent and total personal income are also meaningful data. Rent and income can analyze the disposable income of people in each region, and people with different incomes will have different attitudes to the programs of different political parties. For example, if a party supports tax increases for high-income groups and tax cuts for low-income groups, people with low disposable incomes will maintain a higher support rate for the party. At the same time, because of the high housing prices and incomes in a certain area, there are many high-income groups in this area. Based on such data, we can study the potential relationship between a certain income group and the support rate of a certain party.
  
  
## Assignment Structure

In the first part:
  We have explained the reasons for the selection of the three factors. Next we will use the method on the lab to clean up the data. 
  
  After this we will use the describe() function to observe the mean and standard deviation of the three factors. And use the ggplot function to view the distribution of the data, which gives us a full understanding of the data we want to operate.
  
In the second part:
  The data will be classified according to political parties, and five tables are generated. After that, the glm() function and the Poisson distribution are used to study the relationship between the independent variable and the dependent variable (support rate).
  
  Generate a map based on the variables we selected. For example, the age distribution of people who support the Labor Party, this map is used as the independent variable map. The dependent variable map will be the distribution of the Labor Partyâ€™s approval ratings.
  
  Finally, we will draw our conclusions based on the generated map.


# Part I

## Data Preparetion

  Data cleaning strategy we used in the lab9

### Downloading and analysing spatial data for elections

\vspace{6mm}

```{r function to download booth data, eval=FALSE}


dl.booth.data <- function(x){
  
  read.csv(paste0('https://results.aec.gov.au/24310/Website/Downloads/
                  HouseStateFirstPrefsByPollingPlaceDownload-24310-', x, '.csv'), 
           skip = 1)
  
}

```

```{r function to download booth data2, include=FALSE}


dl.booth.data <- function(x){
  
  read.csv(paste0('https://results.aec.gov.au/24310/Website/Downloads/HouseStateFirstPrefsByPollingPlaceDownload-24310-', x, '.csv'), skip = 1)
  
}

```
\vspace{6mm}

Write out code that downloads and binds the polling place results for each state: 

\vspace{6mm}

```{r load and code 2019 booth data}

booth.results.2019 <- bind_rows(dl.booth.data('NSW'),
                                dl.booth.data('VIC'),
                                dl.booth.data('QLD'),
                                dl.booth.data('WA'),
                                dl.booth.data('SA'),
                                dl.booth.data('TAS'),
                                dl.booth.data('ACT'),
                                dl.booth.data('NT'))

```

\vspace{6mm}

In its raw form this needs some work, though. For instance, there are a lot of small parties who only contested a handful of seats in the election. You can see this for yourself with the syntax: 

\vspace{6mm}

```{r check party coding, eval=FALSE}

table(booth.results.2019$PartyNm)

```

\vspace{6mm}

Therefore we recode the variable `PartyNm`, saving this as `party`: 

\vspace{6mm}

```{r recode party}

booth.results.2019 <- booth.results.2019 %>% 
  mutate(party = dplyr::recode(PartyNm,
                               "Liberal" = 'Coalition',
                               "The Greens" = 'Greens',
                               "Labor" = 'Labor',
                               "Informal" = 'Informal',
                               "Pauline Hanson's One Nation" = 'One Nation',
                               "The Nationals"  = 'Coalition',
                               "The Greens (VIC)" = 'Greens',                    
                               "Australian Labor Party"  = 'Labor',
                               "Liberal National Party of Queensland" = 'Coalition',
                               "The Greens (WA)" = 'Greens',
                               "National Party" = 'Coalition',
                               "Australian Labor Party (Northern Territory) Branch"  = 
                                 'Labor',
                               "Country Liberals (NT)" = 'Coalition',
         .default = 'Other')) %>%
  filter(party != 'Informal')
  
```

\vspace{6mm}

Roll up party votes at each polling place and clean up some of the polls with missing data using `mutate_at`

\vspace{6mm}

```{r calculate proportions for each polling place, message=FALSE, warning=FALSE}

booth.results.2019 <- booth.results.2019 %>% 
group_by(PollingPlaceID, party) %>%
  dplyr::summarise(n = sum(OrdinaryVotes)) %>%
  spread(party, n) %>%
  mutate_at(vars(-PollingPlaceID), 
                   .funs = list(missing = ~ ifelse(is.na(.), 'Yes', 'No'))) %>% 
  mutate_at(vars(-PollingPlaceID), ~ifelse(is.na(.), 0, .))


```

\vspace{6mm}

`gather` to organise the data: 

\vspace{6mm}

```{r other stuff}

  
booth.results.2019 <- booth.results.2019 %>% 
    gather(party, n, 
           Coalition, Greens, Labor, `One Nation`, Other) %>% 
    dplyr::select(PollingPlaceID, party, n) %>% 
    merge(booth.results.2019 %>% 
            gather(party, missing, 
                   Coalition_missing, Greens_missing, 
                   Labor_missing, `One Nation_missing`,
                   Other_missing) %>%
            mutate(party = gsub('_missing', '', party)) %>% 
            dplyr::select(PollingPlaceID, party, missing)) %>% 
  group_by(PollingPlaceID) %>%
  mutate(prop = n / sum(n))

```

\vspace{6mm}

Calculated the proportion of votes at each polling place received by each party saved as `prop`. 

Merging booth information from the AEC on to data frame, and then removing rows with missing data. 

\vspace{6mm}

```{r booth information, eval=FALSE}
  
booth.results.2019 <- booth.results.2019 %>% 
  merge(read.csv('https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv', skip = 1) %>%
            dplyr::rename(state = State, 
                          division = DivisionNm,
                          PollingPlace = PollingPlaceNm,
                          POA_CODE16 = PremisesPostCode) %>% 
  dplyr::select(state, division, PollingPlaceID, PollingPlace,
                POA_CODE16, Latitude, Longitude)) %>% 
  filter(!is.na(Latitude)) %>% 
  filter(!PollingPlace %in% grep('PREPOLL',
                                 PollingPlace, 
                                 value = T)) 
  

```

```{r booth information2, include=FALSE}
  
booth.results.2019 <- booth.results.2019 %>% 
  merge(read.csv('https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv', skip = 1) %>%
            dplyr::rename(state = State, 
                          division = DivisionNm,
                          PollingPlace = PollingPlaceNm,
                          POA_CODE16 = PremisesPostCode) %>% 
  dplyr::select(state, division, PollingPlaceID, PollingPlace,
                POA_CODE16, Latitude, Longitude)) %>% 
  filter(!is.na(Latitude)) %>% 
  filter(!PollingPlace %in% grep('PREPOLL',
                                 PollingPlace, 
                                 value = T)) 
  

```


\vspace{6mm}


### Building on data

\vspace{6mm}


```{r read and merge datapacks, eval=FALSE}

booth.results.2019 <- booth.results.2019 %>% 
  left_join(read.csv('Data/Australia/abs data/2016Census_G02_AUS_POA.csv') %>% 
  mutate(POA_CODE16 = as.numeric(gsub('POA', '', POA_CODE_2016))) %>% 
  dplyr::select(-POA_CODE_2016))


```


```{r read and merge datapacks2, include=FALSE}

booth.results.2019 <- booth.results.2019 %>% 
  left_join(read.csv('Data/Australia/abs data/2016Census_G02_AUS_POA.csv') %>% 
  mutate(POA_CODE16 = as.numeric(gsub('POA', '', POA_CODE_2016))) %>% 
  dplyr::select(-POA_CODE_2016))

```

\vspace{6mm}

## Finilizing data cleaning and descriptive analysis


```{r overview the varaibles we selected and plot distribution graph}

variable <- c('Median_age_persons','Median_rent_weekly','Median_tot_prsnl_inc_weekly')

library(psych)

describe(booth.results.2019[variable])

library(ggplot2)
ggplot(data = booth.results.2019) +
  geom_histogram(mapping = aes(x = Median_age_persons), binwidth = 2)

ggplot(data = booth.results.2019) +
  geom_histogram(mapping = aes(x = Median_rent_weekly), binwidth = 30)

ggplot(data = booth.results.2019) +
  geom_histogram(mapping = aes(x = Median_tot_prsnl_inc_weekly), binwidth = 50)



```

### Analysis of part I
  According to the data and graph given above, the median age of the voters has a mean of 39.61 and a standard deviation of 6.07 the data of median age is little dispersed,min is 18.00 and max is 67.00. the distribution shows a right skew (skewness +0.48) and is slightly flatter than the normal distribution (kurtosis 0.33)
The mean of voters weekly rent is 325.39 and a standard deviation of 120.13 the weekly rent is dispersed,min is 0 and max is 830. the distribution shows a right skew (skewness +0.53) and is slightly steeper than the normal distribution (kurtosis 1.18)
The mean of voters weekly personal income is 671.82 and a standard deviation of 178.50 the personal income is large dispersed,min is 185 and max is 2772. the distribution shows a right skew (skewness +1.66) and is steeper than the normal distribution (kurtosis 5.82)
From the graph1, we can see that the age pf most voters are between 35 to 40 and about 34 is the most. From the graph2, we can see that most voter are rent between 250 to 375 and nearly 375 is the most. Form the graph3, we can see that most voters personal income is between 500 to 750 and nearly 650 is the most.

# Part II 

## Regression model

  In order to eliminate data differences and improve model accuracy, we decided to use the scale() function to centralize and standardize the data.
  After that, the data supporting each party will be selected.Using the glm() function to build a generalized linear regression model for the data of each party. Then use the Poisson distribution to explain the relationship between the independent variable and the dependent variable.

```{r fit the first linear model to county, warning=FALSE, message=FALSE, cache=FALSE}

library(tidyverse)
booth.results.2019.Coalition <- booth.results.2019 %>%
  mutate(z.age = scale(Median_age_persons)) %>%
  mutate(z.rent = scale(Median_rent_weekly)) %>% 
  mutate(z.income = scale(Median_tot_prsnl_inc_weekly)) %>% 
  filter(party == "Coalition")

booth.results.2019.Greens <- booth.results.2019 %>%
  mutate(z.age = scale(Median_age_persons)) %>%
  mutate(z.rent = scale(Median_rent_weekly)) %>% 
  mutate(z.income = scale(Median_tot_prsnl_inc_weekly)) %>% 
  filter(party == "Greens")

booth.results.2019.Labor <- booth.results.2019 %>% 
  mutate(z.age = scale(Median_age_persons)) %>%
  mutate(z.rent = scale(Median_rent_weekly)) %>% 
  mutate(z.income = scale(Median_tot_prsnl_inc_weekly)) %>% 
  filter(party == "Labor")

booth.results.2019.OneNation <- booth.results.2019 %>% 
  mutate(z.age = scale(Median_age_persons)) %>%
  mutate(z.rent = scale(Median_rent_weekly)) %>% 
  mutate(z.income = scale(Median_tot_prsnl_inc_weekly)) %>% 
  filter(party == "One Nation")

booth.results.2019.Other <- booth.results.2019 %>% 
  mutate(z.age = scale(Median_age_persons)) %>%
  mutate(z.rent = scale(Median_rent_weekly)) %>% 
  mutate(z.income = scale(Median_tot_prsnl_inc_weekly)) %>% 
  filter(party == "Other")


library(arm)
booth.model.coliation <- glm(prop ~ z.age + z.rent 
                     + z.income,
                     family=poisson(link = log),
                     data=booth.results.2019.Coalition)

booth.model.greens <- glm(prop ~ z.age + z.rent 
                     + z.income,
                     family=poisson(link = log),
                     data=booth.results.2019.Greens)

booth.model.labor <- glm(prop ~ z.age + z.rent 
                     + z.income,
                     family=poisson(link = log),
                     data=booth.results.2019.Labor)

booth.model.onenation <- glm(prop ~ z.age + z.rent 
                     + z.income,
                     family=poisson(link = log),
                     data=booth.results.2019.OneNation)

booth.model.other <- glm(prop ~ z.age + z.rent 
                     + z.income,
                     family=poisson(link = log),
                     data=booth.results.2019.Other)

display(booth.model.coliation)
display(booth.model.greens)
display(booth.model.labor)
display(booth.model.onenation)
display(booth.model.other)


```

## Analysis of the glm

The fitting poisson model examines the coffecient of the independent(the median age, weekly rent, weekly personal income of voters)and dependent variable(probability of election).In the Poisson regression, the dependent variable is modeled in the logarithmic form of the conditional mean ln(Î»). The poisson model is (prop ~ z.age + z.rent + z.income).

  For the coalition party, the intercept is -0.88, the regression parameter for z.age is 0.13, indicating that holding the other predictor variables constant, a one-year increase in age will result in a corresponding increase in the log mean of the number of election probability by 0.13.The the regression parameter for z.rent is -0.04, a fee of rent increase will result in a corresponding decrease in the log mean of the number of election probability by 0.04.The regression parameter for z.income is 0.07,  increase in personal income will result in a corresponding increase in the log mean of the number of election probability by 0.07.

  For the greens party, the intercept is -2.32, the regression parameter for z.age is -0.06, indicating that holding the other predictor variables constant, a one-year increase in age will result in a corresponding decrease in the log mean of the number of election probability by 0.06.The the regression parameter for z.rent is 0.18, a fee of rent increase will result in a corresponding increase in the log mean of the number of election probability by 0.18.The regression parameter for z.income is 0.10,  increase in personal income will result in a corresponding increase in the log mean of the number of election probability by 0.10.

  For the Labor party, the intercept is -1.16, the regression parameter for z.age is -0.17, indicating that holding the other predictor variables constant, a one-year increase in age will result in a corresponding decrease in the log mean of the number of election probability by 0.17.The regression parameter for z.rent is 0.08, a fee of rent increase will result in a corresponding increase in the log mean of the number of election probability by 0.08.The regression parameter for z.income is -0.12, increase in personal income will result in a corresponding decrease in the log mean of the number of election probability by 0.12.
  
  For the OneNation party, the intercept is -3.51, the regression parameter for z.age is -0.19, indicating that holding the other predictor variables constant, a one-year increase in age will result in a corresponding decrease in the log mean of the number of election probability by 0.19.The regression parameter for z.rent is -0.50, a fee of rent increase will result in a corresponding decrease in the log mean of the number of election probability by 0.50.The regression parameter for z.income is -0.05,  increase in personal income will result in a corresponding decrease in the log mean of the number of election probability by 0.05.
  
  For the other party, the intercept is -2.08, the regression parameter for z.age is 0.09, indicating that holding the other predictor variables constant, a one-year increase in age will result in a corresponding increase in the log mean of the number of election probability by 0.09.The the regression parameter for z.rent is -0.16, a fee of rent increase will result in a corresponding decrease in the log mean of the number of election probability by 0.16.The regression parameter for z.income is 0.02, increase in personal income will result in a corresponding increase in the log mean of the number of election probability by 0.02.
  
  
  
## Data Cleaning and Opearting
  The purpose of the code in this section is to select the appropriate data (zip code and three independent variables) and merge.
  
  We have some special findings when operating on dependent variables.(See below)

  Mention: The values of independent variables in the same postal code are the same, so the following discussion only focuses on the dependent variable (support rate).
  
  Because we found that there are regions with the same zip code in the data, their latitude and longitude are the same, and their area is the same. So in the final result, these locations will be displayed in the same area. In order to reduce unnecessary troubles and the running speed of the data, we decided to clean up and operate the data of the same geographic location and zip code.
  
The theory of our method is:
  Areas in the same postcode have the same geographic location, but there are different voting locations. So we will calculate the total number of voters in each polling station and add them together as the total number of voters in the same postcode area. Then we create a new column of data as the area with the same geographic location and postcode. The number of people supported by this party accounts for the total number of people in the area. The percentage of voters. Compared with before using this method, this operation reduces the drawing time and improves work efficiency. And keep the data results and generated maps consistent with expectations.
  

```{r read shp file and merge with existing data}
library(sf)

au.shape.1 <- st_read("Data/Australia/Au shapefiles/POA_2016_AUST.shp")

# Data for Coalition
Coalition.independent.table <- booth.results.2019 %>%
  filter(party == "Coalition") %>%
  dplyr::select(POA_CODE16, party, Median_age_persons, Median_rent_weekly, Median_tot_prsnl_inc_weekly)

## Remove repeated data
Coalition.independent.table <- dplyr::distinct(Coalition.independent.table)

## Merge data by postcode
Coalition.independent.table <- merge(au.shape.1,
  Coalition.independent.table,
  by="POA_CODE16", duplicateGeoms = TRUE)


```
```{r clean and merge for denpendent varaible prop}

#coalition dependent variable table 
Coalition.dependent.table<-booth.results.2019 %>%
  filter(party == "Coalition" )%>%
  mutate(sumOfVotes.per.pollingPlace = n/prop)%>%
  dplyr::select(POA_CODE16, party, n,  sumOfVotes.per.pollingPlace)%>%
   group_by(POA_CODE16)%>%
  summarise(totalVotes.Coalition = sum(n, na.rm = TRUE), totalVotes = sum(sumOfVotes.per.pollingPlace, na.rm = TRUE))%>%
  mutate(prop.within.samePostCode = (totalVotes.Coalition/totalVotes))

Coalition.dependent.table$prop.within.samePostCode[is.na(Coalition.dependent.table$prop.within.samePostCode)] <- 0

Coalition.dependent.table <-merge(au.shape.1,
  Coalition.dependent.table,
  by="POA_CODE16", duplicateGeoms = TRUE)


```

## Map Plotting
  In this part we spent 2 maps, 1 map about the independent variable age of Coalition party, the distribution in Australia. Another show the distribution of the support rate (dependent variable) of Coalition party in Australia. We will discuss and write down findings based on these figures.

```{r Coalition/Age}

library(mapdeck)

key <- 'pk.eyJ1IjoiZmFuc29uZzk5NiIsImEiOiJja2tvemQ1aWcxa3o3MnBvY3JjeXdmaW1yIn0.1TaYyPHIjaCvbeL6bfJFug'

mapdeck(token = key, style = mapdeck_style("light")) %>%
  add_polygon(
    data = Coalition.independent.table, 
   layer = "polygon_layer", 
   fill_colour = "Median_age_persons", 
   palette = "diverge_hsv", 
   fill_opacity = .9, 
  legend = TRUE)

```


## Map dependent variable for the party Coalition
```{r Coalition/prop}

Coalition.dependent.table <- Coalition.dependent.table %>%
mutate(prop.within.samePostCode2 = cut(prop.within.samePostCode, breaks=c(quantile(prop.within.samePostCode,
probs = seq(0, 1, by = 0.20))),
labels=c("0-20","20-40","40-60","60-80","80-100"), include.lowest=TRUE))


key <- 'pk.eyJ1IjoiZmFuc29uZzk5NiIsImEiOiJja2tvemQ1aWcxa3o3MnBvY3JjeXdmaW1yIn0.1TaYyPHIjaCvbeL6bfJFug'

library(mapdeck)

mapdeck(token = key, style = mapdeck_style("light")) %>%
  add_polygon(
    data = Coalition.dependent.table, 
   layer = "polygon_layer", 
   fill_colour = "prop.within.samePostCode2", 
   palette = "diverge_hsv", 
   fill_opacity = .9, 
  legend = TRUE,
  legend_options = list(title = "Vote Proportion For Coalition"))  

```


### Key findings

  From the generated map, we have findings:
 
  Among those who support Coalition, the median age in northern and central Australia is relatively low, and the median age in the east and west coasts of Australia is high. The median age in the surrounding area of Adelaide is very high, close to 60 years old. The median age in Brisbane is also very high. The median age in northern Western Australia and northern Queensland is very low, between 25-30 years old.
 
  We tried to make maps of the median age of other parties, and we found the results were the same. Through discussion and analysis, we have come to the conclusion: This map reflects to some extent the age distribution of the voting population in Australia. The median population in Northern Australia is low, and the median population in the southern coastal areas (economically developed areas) is high. We analyze this may be because young people who worked and settled in these economically developed areas a few decades ago have a higher median age. The northern coastal area is an emerging development area, where there are job opportunities suitable for young people, and because of government policies and other reasons, the population here is relatively low.
 
  By studying the maps generated by the dependent variables, we found that the Alliance Party has the highest support rates in Western Australia, Queensland and the interior of New South Wales, as high as 80% to 100%. After analysis, we came to the conclusion: Because these areas have rich mineral resources, the Coalition Party may have given people in these areas better policies, allowing them to create more wealth. Therefore, the inland population of Australia has a high support rate for the Coalition Party.
 
  In addition, we have some technical discoveries that in the process of making maps, if the geographic location of the data is repeated, the map generation may be slow or not generated. Therefore, we used an algorithm to calculate and synthesize the data of the same geographic location and the same zip code, so as to optimize the map generation speed without affecting the results and improve our work efficiency.
  
  
